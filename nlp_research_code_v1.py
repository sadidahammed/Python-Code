# -*- coding: utf-8 -*-
"""NLP Research Code V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ai58_WXne9ROxGLenZruFr-yRlLaheag
"""

import os
import pandas as pd

import nltk

# Download the punkt tokenizer
nltk.download('punkt')

import os
import re
import nltk
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, classification_report
from nltk.tokenize import word_tokenize

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
from nltk.tokenize import word_tokenize

from google.colab import drive
drive.mount('/content/drive')

"""## **Dataset was ZIP imports and extract that**"""

zip_path = '/content/drive/MyDrive/NLP/Artical_Dataset.zip'

import zipfile

zip_path = '/content/drive/MyDrive/NLP/Artical_Dataset.zip'
extraction_path = '/content/extracted_files'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extraction_path)

main_folder = '/content/extracted_files'

"""## **Pre Processing**"""

# Define a basic stemming function for Bangla (customized rule-based stemming)
def bangla_stemmer(word):
    suffixes = ['তে', 'তো', 'ি', 'র', 'ের', 'ে', 'ের', 'র']  # Common Bangla suffixes
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

# Define a preprocessing function for Bangla text
def preprocess_bangla_text(text):
    if not text.strip():  # Check for empty or null text
        return ""

    # Lowercasing
    text = text.lower()

    # Removing URLs and emails
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\S+@\S+', '', text)

    # Removing digits and punctuation
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Stop words removal
    bangla_stop_words = ['এর', 'এবং', 'যদি', 'তবে', 'কিন্তু']  # Add common Bangla stop words
    tokens = [word for word in tokens if word not in bangla_stop_words]

    # Stemming
    tokens = [bangla_stemmer(word) for word in tokens]

    # Rejoining tokens to form processed text
    processed_text = ' '.join(tokens)
    return processed_text

# Initialize lists to store the text data and corresponding labels
texts = []
labels = []

# Read all text files from the folder structure
for label in os.listdir(main_folder):
    folder_path = os.path.join(main_folder, label)
    if os.path.isdir(folder_path):
        # Loop through each text file in the class folder
        for file_name in os.listdir(folder_path):
            if file_name.endswith('.txt'):
                file_path = os.path.join(folder_path, file_name)

                # Read the content of the text file
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read().strip()
                    if text:  # Ensure text is not empty
                        texts.append(preprocess_bangla_text(text))  # Preprocess the text and add to list
                        labels.append(label)  # Add the corresponding label (folder name)

# Check if any texts were loaded
if not texts:
    print("No valid text data found in the dataset. Please check your folders.")
else:
    print(f"Loaded {len(texts)} texts from the dataset.")

"""## **Train Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)

"""## **Apply Naive Bayes classifier**"""

# Convert text data into feature vectors
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Initialize and train the Naive Bayes classifier
classifier = MultinomialNB()
classifier.fit(X_train_vectorized, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test_vectorized)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", report)

"""## **Apply Naive Bayes Classifier with tunning**"""

# Convert text data into feature vectors using TfidfVectorizer with n-grams
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=5)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Tune the Naive Bayes classifier using GridSearchCV
# We can tune the 'alpha' parameter of MultinomialNB
param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]}
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_vectorized, y_train)

# Get the best model after tuning
best_classifier = grid_search.best_estimator_

# Make predictions on the test set with the best model
y_pred = best_classifier.predict(X_test_vectorized)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Best alpha: {grid_search.best_params_['alpha']}")
print(f"Tuned Accuracy: {accuracy:.4f}")
print("Tuned Classification Report:\n", report)

"""# *Improve accuracy 0.939 to 0.9494*"""

# Step 5: Tune the Naive Bayes classifier using GridSearchCV
# We can tune the 'alpha' parameter of MultinomialNB
param_grid = {
    'alpha': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0],  # More fine-grained range for alpha
    'fit_prior': [True, False],  # Test both options for learning class prior probabilities
    'class_prior': [None, [0.25, 0.25, 0.25, 0.25]],  # Manually specify class priors (if known)
}
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_vectorized, y_train)

# Get the best model after tuning
best_classifier = grid_search.best_estimator_

# Step 6: Make predictions on the test set with the best model
y_pred = best_classifier.predict(X_test_vectorized)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Best alpha: {grid_search.best_params_['alpha']}")
print(f"Tuned Accuracy: {accuracy:.4f}")
print("Tuned Classification Report:\n", report)

"""# *Improve accuracy 0.939 to 0.9524*"""

import matplotlib.pyplot as plt

# Step 8: Plot histograms for the training and test predictions
# Plotting histogram for predicted labels in the training set
plt.figure(figsize=(10, 6))

# Training set predictions
y_train_pred = best_classifier.predict(X_train_vectorized)
plt.subplot(1, 2, 1)
plt.hist(y_train_pred, bins=30, color='blue', edgecolor='black', alpha=0.7)
plt.title('Training Set Predictions')
plt.xlabel('Labels')
plt.ylabel('Frequency')

# Test set predictions
plt.subplot(1, 2, 2)
plt.hist(y_pred, bins=30, color='green', edgecolor='black', alpha=0.7)
plt.title('Test Set Predictions')
plt.xlabel('Labels')
plt.ylabel('Frequency')

# Show the plots
plt.tight_layout()
plt.show()

"""## **SVM Models apply**"""

from sklearn.svm import SVC  # Import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert text data into feature vectors using TfidfVectorizer with n-grams
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=5)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

#Initialize the SVM classifier (no tuning)
svm_classifier = SVC(random_state=42)

# Train the SVM model on the training data
svm_classifier.fit(X_train_vectorized, y_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test_vectorized)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"SVM Accuracy: {accuracy:.4f}")
print("SVM Classification Report:\n", report)

"""## **Apply Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize the Random Forest Classifier without tuning
rf_classifier = RandomForestClassifier(random_state=42)

# Train the Random Forest model on the training data
rf_classifier.fit(X_train_vectorized, y_train)

# Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test_vectorized)

# Evaluate the model performance
accuracy_rf = accuracy_score(y_test, y_pred_rf)
report_rf = classification_report(y_test, y_pred_rf)

print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print("Random Forest Classification Report:\n", report_rf)

# Create confusion matrix for Random Forest model
cm_rf = confusion_matrix(y_test, y_pred_rf)

# Plot confusion matrix for Random Forest
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.title('Random Forest Classifier - Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()